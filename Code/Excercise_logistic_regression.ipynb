{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import texttable as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10ade2a90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 19241924\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the dataset with the helper function `load_compas_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for file 'compas-scores-two-years.csv' in the current directory...\n",
      "File found in current directory..\n",
      "\n",
      "Number of people recidivating within two years\n",
      "-1    2795\n",
      " 1    2483\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Features we will be using for classification are: ['intercept', 'age_cat_25 - 45', 'age_cat_Greater than 45', 'age_cat_Less than 25', 'race', 'sex', 'priors_count', 'c_charge_degree'] \n",
      "\n",
      "Number of samples 5278, number of features 8\n"
     ]
    }
   ],
   "source": [
    "from load_compas_data import load_compas_data\n",
    "x, y, x_sens = load_compas_data()\n",
    "print(f'Number of samples {x.shape[0]}, number of features {x.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Selecting sensitive feature\n",
    "x_sens = x_sens['race']\n",
    "race_type = {'white': 0, 'black': 1} # TODO check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is of the form: $Y = \\sigma (X \\theta + b)$ where $X$ is a matrix of size $N\\times F$, $\\theta$ of size $F\\times1$, $b$ is a scalar value and $\\sigma(\\cdot)$ is the sigmoid function, i.e., $\\sigma(x) = (1+\\exp(-x))^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to simplify the calculi, we are going to write $X\\theta + b$ as $\\left[X~1\\right] \\left[\\theta^T b\\right]^T$. \n",
    "\n",
    "That is, we are going to embed the bias value into the set of parameters $\\theta$ and add a column of ones to $X$. By calling $X=\\left[X~1\\right]$ and $\\theta=\\left[\\theta^T b\\right]^T$ our model is written as $Y=\\sigma(X\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.c_[x, np.ones((x.shape[0],1))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are also going to transform $Y$ such that it contains 0s and 1s instead of 1s and -1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "y = (y + 1)/2\n",
    "print(y[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are going to work with Pytorch we need to transform the data to the right data type. In this case, we have to transform every numpy object to Pytorch's tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(x, dtype=torch.float)\n",
    "y = torch.tensor(y, dtype=torch.float)\n",
    "x_sens = torch.tensor(x_sens, dtype=torch.uint8)\n",
    "\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are going to split the data into train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(x.size(0) * 0.8)\n",
    "random_perm = torch.randperm(x.size(0))\n",
    "train_idx, test_idx = random_perm[:train_size], random_perm[train_size:]\n",
    "\n",
    "x_train, x_test = x[train_idx], x[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "x_sens_train, x_sens_test = x_sens[train_idx], x_sens[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to define some measures that we will use throughtout the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demographic_parity(y, mask):\n",
    "    #TODO\n",
    "    return 0 #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_opportunity(y_pred, y, mask):\n",
    "    #TODO\n",
    "    return 0 #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_positive(y, mask):\n",
    "    y_cond = y * mask.float()\n",
    "    return y_cond.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y, mask):\n",
    "    y_cond = mask.float() * ( y * y_pred + (1 - y) * (1 - y_pred))\n",
    "    return y_cond.sum() / mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "+----------------+--------+--------+\n",
      "|                | white  | black  |\n",
      "+================+========+========+\n",
      "| positive class | 31.79% | 15.47% |\n",
      "+----------------+--------+--------+\n",
      "| negative class | 28.49% | 24.25% |\n",
      "+----------------+--------+--------+\n",
      "Test:\n",
      "+----------------+--------+--------+\n",
      "|                | white  | black  |\n",
      "+================+========+========+\n",
      "| positive class | 30.21% | 16.00% |\n",
      "+----------------+--------+--------+\n",
      "| negative class | 29.45% | 24.34% |\n",
      "+----------------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "to_percentage = lambda l: [f'{float(y) * 100.:.2f}%' for y in l]\n",
    "\n",
    "for name, [x, y, x_sens] in zip(['Train', 'Test'], [[x_train, y_train, x_sens_train], [x_test, y_test, x_sens_test]]):\n",
    "    table = tt.Texttable()\n",
    "    table.header([''] + list(race_type.keys()))\n",
    "\n",
    "    \n",
    "    table.add_row(['positive class'] + to_percentage([num_positive(y, x_sens == v) for v in race_type.values()]))\n",
    "    table.add_row(['negative class'] + to_percentage([num_positive(1 - y, x_sens == v) for v in race_type.values()]))\n",
    "    #table.add_row(['Benefits demographic parity'] # + TODO\n",
    "\n",
    "    print(f'{name}:')\n",
    "    print(table.draw())\n",
    "    \n",
    "    #print(f'Demographic Disparity = {TODO}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's define our logistic regression model. In Pytorch this is done by creating a subclass of `torch.nn.Module` which define its parameters at `__init__` and acts on the input through the function `forward`. We will initialize our parameters randomly using a $\\mathcal{N}(0, 1)$ distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.theta = nn.Parameter(torch.randn(size, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(torch.matmul(x, self.theta)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's declare some helper functions to see all the info we care about or model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_pred, y, mask=None):\n",
    "    size = y.size(0) if mask is None else mask.sum().item()\n",
    "    sum_and_int = lambda x: x.sum().long().item()\n",
    "    to_percentage = lambda l: [f'{float(y)* 100. / size :.2f}%' for y in l]\n",
    "    \n",
    "    y_pred_binary = (y_pred > 0.5).float()\n",
    "    if mask is None:\n",
    "        mask = torch.ones_like(y_pred)\n",
    "    mask = mask.float()\n",
    "    \n",
    "    true_positives = sum_and_int(y * y_pred_binary * mask)\n",
    "    false_positives = sum_and_int((1 - y) * y_pred_binary * mask)\n",
    "    \n",
    "    true_negatives = sum_and_int((1 - y) * (1 - y_pred_binary) * mask)\n",
    "    false_negatives = sum_and_int(y * (1 - y_pred_binary) * mask)\n",
    "    \n",
    "    total_positives = true_positives + false_negatives\n",
    "    total_negatives = true_negatives + false_positives\n",
    "    total = total_positives + total_negatives\n",
    "    \n",
    "    # Show the confusion matrix\n",
    "    table = tt.Texttable()\n",
    "    table.header(['Pred/Real', 'Positive', 'Negative', ''])\n",
    "    table.add_row(['Positive'] + to_percentage([true_positives, false_positives, true_positives + false_positives]))\n",
    "    table.add_row(['Negative'] + to_percentage([false_negatives, true_negatives, false_negatives + true_negatives]))\n",
    "    table.add_row([''] + to_percentage([total_positives, total_negatives, total]))\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measures_table(y_pred, y, x_sens):\n",
    "    to_percentage = lambda l: [f'{float(y) * 100.:.2f}%' for y in l]\n",
    "    y_pred_binary = (y_pred > 0.5).float()\n",
    "    \n",
    "    table = tt.Texttable()\n",
    "    table.header([''] + list(race_type.keys()))\n",
    "    \n",
    "    \n",
    "    table.add_row(['accuracy'] + to_percentage([accuracy(y_pred_binary, y, x_sens == v) for v in race_type.values()]))\n",
    "    table.add_row(['positive class'] + to_percentage([num_positive(y_pred_binary, x_sens == v) for v in race_type.values()]))\n",
    "    table.add_row(['negative class'] + to_percentage([num_positive(1 - y_pred_binary, x_sens == v) for v in race_type.values()]))\n",
    "    #table.add_row(['benefit demographic parity'] + TODO\n",
    "    #table.add_row(['benefit equal opportunity'] + TODO\n",
    "    \n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # To avoid calculating any gradients for the parameters\n",
    "def print_model_info(model, test=True, samples=20):\n",
    "    if test:\n",
    "        print('### Test ###')\n",
    "        x, y, x_sens = x_train, y_train, x_sens_train\n",
    "    else:\n",
    "        print('### Train ###')\n",
    "        x, y, x_sens = x_test, y_test, x_sens_test\n",
    "        \n",
    "    theta = model.theta.flatten()[:-1]\n",
    "    print(f'Theta:            {theta.tolist()}')\n",
    "    \n",
    "    bias = model.theta.flatten()[-1]\n",
    "    print(f'Bias:             {bias.tolist()}')\n",
    "    \n",
    "    y_pred = model(x)\n",
    "    y_pred_binary = (y_pred > 0.5).float()\n",
    "    print()\n",
    "    print(f'Predictions:      {[round(s, 1) for s in y_pred[:samples].tolist()]}')\n",
    "    print(f'Binarized preds:  {y_pred_binary[:samples].tolist()}')\n",
    "    print(f'Targets:          {y[:samples].flatten().tolist()}')\n",
    "\n",
    "    # Build the confusion matrix\n",
    "    table = confusion_matrix(y_pred, y)    \n",
    "    print('Confusion matrix:')\n",
    "    print(table.draw())\n",
    "    \n",
    "    for k, v in race_type.items():\n",
    "        table = confusion_matrix(y_pred, y, x_sens == v)\n",
    "        print(f'Confusion matrix for race={k}:')\n",
    "        print(table.draw())\n",
    "        \n",
    "    # Print different measures\n",
    "    table = measures_table(y_pred, y, x_sens)\n",
    "    print('Fairness measures')\n",
    "    print(table.draw())\n",
    "    print()\n",
    "    print(f'Demographic Disparity = #TODO')\n",
    "    print(f'Equal Opportinity = #TODO')\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the gradient descent method, that is, update our parameters on the opossite direction of the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # This is to let Pytorch modify our parameters\n",
    "def gradient_descent(model, learning_rate):\n",
    "    model.theta -= learning_rate * model.theta.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_function, x, y, epochs, learning_rate, print_freq=100, **kwargs):\n",
    "    for i in range(epochs):\n",
    "        model.zero_grad()  # sets all the gradients back to 0\n",
    "    \n",
    "        y_pred = model(x)\n",
    "        loss = loss_function(y_pred, y, **kwargs)\n",
    "        if i == 0 or (i+1) % print_freq == 0:\n",
    "            print(f'Epoch {i+1}: loss {loss.item()}')\n",
    "            \n",
    "        loss.backward()  # calculates the gradient of all the parameters\n",
    "        gradient_descent(model, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our model and see if something has changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximizing efficiency approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to take the usual approach where we don't consider any fairness constrains and hence we only care about minimizing our loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a loss function we are going to use the average binary cross entropy loss, that is $\\frac{1}{N}\\sum_{n=1}^N \\left[ y_n\\log \\hat{y}_n + (1-y_n)\\log (1-\\hat{y}_n) \\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 0.8671475648880005\n",
      "Epoch 100: loss 0.6692675352096558\n",
      "Epoch 200: loss 0.6501793265342712\n",
      "Epoch 300: loss 0.6396610140800476\n",
      "Epoch 400: loss 0.633036196231842\n",
      "Epoch 500: loss 0.6286677122116089\n",
      "Epoch 600: loss 0.6257181167602539\n",
      "Epoch 700: loss 0.6237037181854248\n",
      "Epoch 800: loss 0.6223220229148865\n",
      "Epoch 900: loss 0.621358335018158\n",
      "Epoch 1000: loss 0.6206861734390259\n"
     ]
    }
   ],
   "source": [
    "my_model_me = LogisticRegression(x.size(1))\n",
    "my_loss_function = nn.functional.binary_cross_entropy  # cross entropy loss\n",
    "\n",
    "train(my_model_me, my_loss_function, x_train, y_train, epochs=1000, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Train ###\n",
      "Theta:            [-0.26064425706863403, 0.15851888060569763, -0.33682048320770264, 0.7693789601325989, -0.27491647005081177, 0.44202113151550293, 0.7048741579055786, -0.24221782386302948]\n",
      "Bias:             -0.18691441416740417\n",
      "\n",
      "Predictions:      [0.4, 0.3, 0.6, 0.6, 0.5, 0.7, 0.2, 0.9, 0.5, 0.3, 0.5, 0.6, 0.8, 0.5, 0.3, 0.8, 0.6, 0.4, 0.4, 0.4]\n",
      "Binarized preds:  [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]\n",
      "Targets:          [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]\n",
      "Confusion matrix:\n",
      "+-----------+----------+----------+---------+\n",
      "| Pred/Real | Positive | Negative |         |\n",
      "+===========+==========+==========+=========+\n",
      "| Positive  | 26.04%   | 13.07%   | 39.11%  |\n",
      "+-----------+----------+----------+---------+\n",
      "| Negative  | 20.17%   | 40.72%   | 60.89%  |\n",
      "+-----------+----------+----------+---------+\n",
      "|           | 46.21%   | 53.79%   | 100.00% |\n",
      "+-----------+----------+----------+---------+\n",
      "Confusion matrix for race=white:\n",
      "+-----------+----------+----------+---------+\n",
      "| Pred/Real | Positive | Negative |         |\n",
      "+===========+==========+==========+=========+\n",
      "| Positive  | 36.51%   | 18.25%   | 54.76%  |\n",
      "+-----------+----------+----------+---------+\n",
      "| Negative  | 14.13%   | 31.11%   | 45.24%  |\n",
      "+-----------+----------+----------+---------+\n",
      "|           | 50.63%   | 49.37%   | 100.00% |\n",
      "+-----------+----------+----------+---------+\n",
      "Confusion matrix for race=black:\n",
      "+-----------+----------+----------+---------+\n",
      "| Pred/Real | Positive | Negative |         |\n",
      "+===========+==========+==========+=========+\n",
      "| Positive  | 10.56%   | 5.40%    | 15.96%  |\n",
      "+-----------+----------+----------+---------+\n",
      "| Negative  | 29.11%   | 54.93%   | 84.04%  |\n",
      "+-----------+----------+----------+---------+\n",
      "|           | 39.67%   | 60.33%   | 100.00% |\n",
      "+-----------+----------+----------+---------+\n",
      "Fairness measures\n",
      "+----------------+--------+--------+\n",
      "|                | white  | black  |\n",
      "+================+========+========+\n",
      "| accuracy       | 67.62% | 65.49% |\n",
      "+----------------+--------+--------+\n",
      "| positive class | 32.67% | 6.44%  |\n",
      "+----------------+--------+--------+\n",
      "| negative class | 26.99% | 33.90% |\n",
      "+----------------+--------+--------+\n",
      "\n",
      "Demographic Disparity = #TODO\n",
      "Equal Opportinity = #TODO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_model_info(my_model_me, test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Test ###\n",
      "Theta:            [-0.26064425706863403, 0.15851888060569763, -0.33682048320770264, 0.7693789601325989, -0.27491647005081177, 0.44202113151550293, 0.7048741579055786, -0.24221782386302948]\n",
      "Bias:             -0.18691441416740417\n",
      "\n",
      "Predictions:      [0.6, 0.5, 0.3, 0.3, 0.7, 0.4, 0.5, 0.6, 0.3, 0.4, 0.3, 0.9, 0.4, 0.6, 0.3, 0.4, 0.4, 0.5, 0.9, 0.4]\n",
      "Binarized preds:  [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "Targets:          [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]\n",
      "Confusion matrix:\n",
      "+-----------+----------+----------+---------+\n",
      "| Pred/Real | Positive | Negative |         |\n",
      "+===========+==========+==========+=========+\n",
      "| Positive  | 27.12%   | 12.96%   | 40.08%  |\n",
      "+-----------+----------+----------+---------+\n",
      "| Negative  | 20.13%   | 39.79%   | 59.92%  |\n",
      "+-----------+----------+----------+---------+\n",
      "|           | 47.25%   | 52.75%   | 100.00% |\n",
      "+-----------+----------+----------+---------+\n",
      "Confusion matrix for race=white:\n",
      "+-----------+----------+----------+---------+\n",
      "| Pred/Real | Positive | Negative |         |\n",
      "+===========+==========+==========+=========+\n",
      "| Positive  | 37.45%   | 17.41%   | 54.85%  |\n",
      "+-----------+----------+----------+---------+\n",
      "| Negative  | 15.28%   | 29.86%   | 45.15%  |\n",
      "+-----------+----------+----------+---------+\n",
      "|           | 52.73%   | 47.27%   | 100.00% |\n",
      "+-----------+----------+----------+---------+\n",
      "Confusion matrix for race=black:\n",
      "+-----------+----------+----------+---------+\n",
      "| Pred/Real | Positive | Negative |         |\n",
      "+===========+==========+==========+=========+\n",
      "| Positive  | 11.45%   | 6.20%    | 17.65%  |\n",
      "+-----------+----------+----------+---------+\n",
      "| Negative  | 27.49%   | 54.86%   | 82.35%  |\n",
      "+-----------+----------+----------+---------+\n",
      "|           | 38.94%   | 61.06%   | 100.00% |\n",
      "+-----------+----------+----------+---------+\n",
      "Fairness measures\n",
      "+----------------+--------+--------+\n",
      "|                | white  | black  |\n",
      "+================+========+========+\n",
      "| accuracy       | 67.31% | 66.31% |\n",
      "+----------------+--------+--------+\n",
      "| positive class | 33.06% | 7.01%  |\n",
      "+----------------+--------+--------+\n",
      "| negative class | 27.21% | 32.71% |\n",
      "+----------------+--------+--------+\n",
      "\n",
      "Demographic Disparity = #TODO\n",
      "Equal Opportinity = #TODO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_model_info(my_model_me, test=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographic parity approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equal opportunity approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum efficiency model:\n",
      "+----------------+--------+--------+\n",
      "|                | white  | black  |\n",
      "+================+========+========+\n",
      "| accuracy       | 67.62% | 65.49% |\n",
      "+----------------+--------+--------+\n",
      "| positive class | 32.67% | 6.44%  |\n",
      "+----------------+--------+--------+\n",
      "| negative class | 26.99% | 33.90% |\n",
      "+----------------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "print('Maximum efficiency model:')\n",
    "print(measures_table(my_model_me(x_test), y_test, x_sens_test).draw())\n",
    "\n",
    "#print('Demographic parity model:')\n",
    "#print(measures_table(my_model_dp(x_test), y_test, x_sens_test).draw())\n",
    "\n",
    "#print('Equal opportunity model:')\n",
    "#print(measures_table(my_model_eo(x_test), y_test, x_sens_test).draw())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
